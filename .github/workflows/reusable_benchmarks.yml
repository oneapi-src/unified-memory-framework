# Executes benchmarks implemented in this repository using scripts
# for results visualization from intel/llvm.
name: Benchmarks

on:
  workflow_call:
    inputs:
      pr_no:
        # even though this is a number, this is a workaround for issues with
        # reusable workflow calls that result in "Unexpected value '0'" error.
        type: string
        default: '0'
      bench_script_params:
        required: false
        type: string
        default: ''
      bench_script_compare:
        required: false
        type: string
        default: ''
      runner:
        required: false
        type: string
        default: 'L0_PERF_PVC'
      compatibility:
        required: false
        type: string
        default: '0'
        description: |
          Set it to 1 to run compatibility sycl benchmarks

permissions:
  contents: read
  pull-requests: read

env:
  UMF_DIR: "${{github.workspace}}/umf-repo"
  BUILD_DIR : "${{github.workspace}}/umf-repo/build"

jobs:
  benchmarks:
    name: Benchmarks
    # run only on upstream; forks will not have the HW
    if: github.repository == 'oneapi-src/unified-memory-framework'
    runs-on: ${{ inputs.runner }}
    permissions:
      contents: write
      pull-requests: write

    steps:
    - name: Establish bench params
      run: |
        params="${{ inputs.bench_script_params }}"
        if [ -n "${{ inputs.bench_script_compare }}" ]; then
          params="$params --compare '${{ inputs.bench_script_compare }}'"
        fi

        echo "params=$params"
        echo "bench_params=$params" >> $GITHUB_ENV

    - name: Add comment to PR
      uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
      if: ${{ always() && inputs.pr_no != 0 }}
      with:
        script: |
          const pr_no = '${{ inputs.pr_no }}';
          const url = '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}';
          const params = `${{ env.bench_params }}`;
          const body = `Compute Benchmarks run (with params: ${params}):\n${url}`;

          github.rest.issues.createComment({
            issue_number: pr_no,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          })

    - name: Checkout UMF
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        path: ${{env.UMF_DIR}}
        fetch-depth: 0

    # We need to fetch special ref for proper PR's merge commit. Note, this ref may be absent if the PR is already merged.
    - name: Fetch PR's merge commit
      if: ${{ inputs.pr_no != 0 }}
      working-directory: ${{env.UMF_DIR}}
      env:
        PR_NO: ${{ inputs.pr_no }}
      run: |
        git fetch -- https://github.com/${{github.repository}} +refs/pull/${PR_NO}/*:refs/remotes/origin/pr/${PR_NO}/*
        git checkout origin/pr/${PR_NO}/merge
        git rev-parse origin/pr/${PR_NO}/merge

    - name: Configure UMF
      run: >
        cmake
        -S ${{env.UMF_DIR}}
        -B ${{env.BUILD_DIR}}
        -DCMAKE_BUILD_TYPE=Release
        -DUMF_BUILD_SHARED_LIBRARY=ON
        -DUMF_BUILD_BENCHMARKS=ON
        -DUMF_BUILD_BENCHMARKS_MT=ON
        -DUMF_BUILD_TESTS=OFF
        -DUMF_BUILD_EXAMPLES=OFF
        -DUMF_DEVELOPER_MODE=OFF
        -DUMF_FORMAT_CODE_STYLE=OFF
        -DUMF_BUILD_LEVEL_ZERO_PROVIDER=ON
        -DUMF_BUILD_CUDA_PROVIDER=ON
        -DUMF_BUILD_LIBUMF_POOL_JEMALLOC=ON

    - name: Build UMF
      run: cmake --build ${{env.BUILD_DIR}} -j $(nproc)

    - name: Checkout UMF results branch
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        ref: benchmark-results
        path: results-repo

    # Get scripts for benchmark data visualization (from SYCL repo).
    # Use specific ref, as the scripts or files' location may change.
    - name: Checkout benchmark scripts
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        repository: intel/llvm
        # Note: The same ref is used in docs build (for dashboard generation)!
        #
        # 15.04.2025
        # branch: sycl
        ref: 08d11bcae0cc2daec903f222c9b3e3af92f3b806
        path: sc
        sparse-checkout: |
          devops/scripts/benchmarks

    - name: Install benchmarking scripts deps
      run: |
        python3 -m venv .venv
        source .venv/bin/activate
        echo "$PATH" >> $GITHUB_PATH
        pip install -r ${{github.workspace}}/sc/devops/scripts/benchmarks/requirements.txt

    - name: Set core range and GPU mask
      run: |
        # On the L0_PERF_PVC runner, compute the core range for the second NUMA node;
        # first node is for SYCL/UR jobs.
        # Skip the first 4 cores - the kernel is likely to schedule more work on these.
        CORES=$(lscpu | awk '
          /NUMA node1 CPU|On-line CPU/ {line=$0}
          END {
            split(line, a, " ")
            split(a[4], b, ",")
            sub(/^0/, "4", b[1])
            print b[1]
          }')
        echo "Selected core: $CORES"
        echo "CORES=$CORES" >> $GITHUB_ENV

        ZE_AFFINITY_MASK=${{ inputs.runner == 'L0_PERF_ARC' && '0' || '1' }}
        echo "ZE_AFFINITY_MASK=$ZE_AFFINITY_MASK" >> $GITHUB_ENV

    - name: Download latest sycl
      if: inputs.compatibility == 1
      run: |
        llvm_tag=$(curl -s https://api.github.com/repos/intel/llvm/releases | awk -F'"' '/"tag_name": "nightly/ {print $4; exit}')
        download_url="https://github.com/intel/llvm/releases/download/${llvm_tag}/sycl_linux.tar.gz"
        echo "llvm tag: $llvm_tag"
        wget --no-verbose $download_url -O sycl_linux.tar.gz

    - name: Unpack sycl
      if: inputs.compatibility == 1
      run: |
        mkdir -p sycl
        tar -xzf sycl_linux.tar.gz -C sycl --strip-components=1
        rm sycl_linux.tar.gz
        echo "SYCL_DIR=${{ github.workspace }}/sycl" >> $GITHUB_ENV
        echo "${{ github.workspace }}/sycl/bin" >> $GITHUB_PATH
        echo "LD_LIBRARY_PATH=${{ github.workspace }}/sycl/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV

    - name: Remove UMF libraries from sycl
      if: inputs.compatibility == 1
      run: rm -f ${{ env.SYCL_DIR }}/lib/libumf*

    - name: Copy UMF libraries to sycl
      if: inputs.compatibility == 1
      run: |
        cp ${{ env.BUILD_DIR }}/lib/libumf* ${{ env.SYCL_DIR }}/lib/

    - name: Run sycl-ls
      if: inputs.compatibility == 1
      env:
        LD_LIBRARY_PATH: ${{ env.SYCL_DIR }}/lib
        SYCL_UR_TRACE: 1
        SYCL_UR_USE_LEVEL_ZERO_V2: 1
      run: ${{ env.SYCL_DIR }}/bin/sycl-ls

    - name: Run benchmarks
      id: benchmarks
      env:
        LD_LIBRARY_PATH: ${{ env.SYCL_DIR }}/lib
        CPATH: ${{ env.SYCL_DIR }}/include
      run: >
        taskset -c ${{ env.CORES }} ./sc/devops/scripts/benchmarks/main.py
        ~/bench_workdir_umf
        --results-dir ${{ github.workspace }}/results-repo
        --output-markdown
        ${{ (inputs.compatibility == 0) && format('--umf {0} --timeout 3000 --output-html remote', env.BUILD_DIR) || '' }}
        ${{ (inputs.compatibility == 1) && format('--sycl {0} --timeout 7200', env.SYCL_DIR) || '' }}
        ${{ env.bench_params }}

    # In case it failed to add a comment, we can still print the results.
    - name: Print benchmark results
      if: ${{ always() }}
      run: cat ${{ github.workspace }}/benchmark_results.md || true

    - name: Add comment to PR
      uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
      if: ${{ always() && inputs.pr_no != 0 }}
      with:
        script: |
          let markdown = ""
          try {
            const fs = require('fs');
            markdown = fs.readFileSync('${{ github.workspace }}/benchmark_results.md', 'utf8');
          } catch(err) {
          }

          const pr_no = '${{ inputs.pr_no }}';
          const url = '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}';
          const test_status = '${{ steps.benchmarks.outcome }}';
          const job_status = '${{ job.status }}';
          const params = `${{ env.bench_params }}`;
          const body = `Compute Benchmarks run (${params}):\n${url}\nJob status: ${job_status}. Test status: ${test_status}.\n ${markdown}`;

          github.rest.issues.createComment({
            issue_number: pr_no,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          })

    - name: Commit data.json and results directory
      working-directory: results-repo
      if: inputs.compatibility == 0
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions@github.com"

        for attempt in {1..5}; do
          echo "Attempt #$attempt to push changes"

          rm -f data.json
          cp ${{ github.workspace }}/sc/devops/scripts/benchmarks/html/data.json .

          git add data.json results/
          git commit -m "Add benchmark results and data.json"

          results_file=$(git diff HEAD~1 --name-only -- results/ | head -n 1)

          if git push origin benchmark-results; then
            echo "Push succeeded"
            break
          fi

          echo "Push failed, retrying..."

          if [ -n "$results_file" ]; then
            mv $results_file ${{ github.workspace }}/temp_$(basename $results_file)

            git reset --hard origin/benchmark-results
            git pull origin benchmark-results

            new_file="results/$(basename "$results_file")"
            mv ${{ github.workspace }}/temp_$(basename $results_file) $new_file
          fi

          echo "Regenerating data.json"
          (cd ${{ github.workspace }} && ${{ github.workspace }}/sc/devops/scripts/benchmarks/main.py ~/bench_workdir_umf --dry-run --results-dir ${{ github.workspace }}/results-repo --output-html remote)

        done
